import requests
from bs4 import BeautifulSoup
from connectToDB import dbConnect
import emoji
import re
from datetime import datetime
from getDateBySelenium import getDynamicData
from getContentBySelenium import getDynamicContentData

def split_count(text):
    return ''.join(c for c in text if c in emoji.UNICODE_EMOJI['en'])

def has_numbers(inputString):
    return any(char.isdigit() for char in inputString)


class crawling:

    def extract_data(version):
        result_hash = {}
        url = f"https://kimchinchips.stibee.com/p/{version}/"
        header = {'User-agent' : 'Mozila/2.0'}
        response = requests.get(url, headers=header)
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        ## ë§Œì•½ ë‰´ìŠ¤ë ˆí„° í˜ì´ì§€ë¥¼ ì°¾ì„ìˆ˜ ì—†ëŠ” ê²½ìš°
        not_found_msg = "ë‰´ìŠ¤ë ˆí„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"
        if not_found_msg in soup.get_text():
            print(not_found_msg)
            return None
        else : 
            
            # title
            whole_title = soup.title.text
            print(whole_title)
            split_title = ''
            if "ğŸŸ" not in whole_title:
                emoji_list = split_count(whole_title)
                split_title = whole_title.strip(emoji_list[1]).split(emoji_list[1])
            else : 
                split_title = whole_title.split("ğŸŸ")

            title = split_title[1][1:]

            # issue_number
            issue_number = ''
            if has_numbers(split_title[0]):
                issue_number  = re.findall(r'\d\.\d|\d+', split_title[0])[0]
                
            else : 
                issue_number = split_title[0][1:]

            # issue date
            issue_date_format = getDynamicData.get_date(version)
            date_format = '%Y. %m. %d.'
            issue_date = datetime.strptime(issue_date_format, date_format)
            print(issue_date)

            # tag 135íšŒë¶€í„° ìˆìŒ
            mail_content_not_found_text = "ì´ ë©”ì¼ì´ ì˜ ì•ˆë³´ì´ì‹œë‚˜ìš”?"
            try:
                tags_parent = soup.find(lambda tag:tag.name=="a" and mail_content_not_found_text in tag.text)
                whole_tags_list = tags_parent.findChild().text.replace(' ','')
                tags_list = whole_tags_list.split("#")[1:]
                print(tags_list)
                result_hash['tags'] = tags_list


            except (AttributeError, NameError):
                print("tags not found")
                result_hash['tags'] = None
                
            # table of content
            def find_table_of_content(table_name):
                result = soup.find(lambda tag:tag.name=="span" and table_name in tag.text)
                if result is None:
                    return None
            
                else :
                    table = result.find_parent().find_parent().text.strip()
                    if len(re.sub(r'ğŸ’¬\s?ì˜¤ëŠ˜ì˜ ê¹€ì¹˜ì•¤ì¹©ìŠ¤', '', table))> 0:
                        return re.sub(r'ğŸ’¬\s?ì˜¤ëŠ˜ì˜ ê¹€ì¹˜ì•¤ì¹©ìŠ¤', '', table)

                    else : 
                        return result.find_parent().find_parent().find_next_sibling().text
                    

            table_of_content = find_table_of_content("ì˜¤ëŠ˜ì˜ ê¹€ì¹˜ì•¤ì¹©ìŠ¤")

            result_text = ''
            ## ì˜ˆì „ ë‰´ìŠ¤ë ˆí„°ëŠ” ëª©ì°¨ê°€ ì˜¤ëŠ˜ì˜ ì£¼ìš” MENUë¼ê³  ë‚˜ì˜´ 
            if table_of_content is None:
                table_menu = soup.find(lambda tag:tag.name=="span" and "ì˜¤ëŠ˜ì˜ ì£¼ìš” MENU" in tag.text)
                if table_menu is not None:
                    table_menu = table_menu.find_parent().find_next_siblings()
                    for r in table_menu:
                        result_text += r.text +"\n"
                    table_of_content = result_text
                
                else : 
                    table_sosik = soup.find(lambda tag:tag.name=="span" and "ì˜¤ëŠ˜ì˜ ì£¼ìš” ì†Œì‹" in tag.text)
                    if table_sosik is not None:
                        table_sosik = table_sosik.find_parent().find_parent().next_siblings
                        for r in table_sosik: 
                            result_text += r.text +"\n"
                        table_of_content = result_text

            # content
            content = soup.find("div", "email-content").prettify()
            #content = getDynamicContentData.get_date(version)

            result_hash['title'] = title
            result_hash['issue_number'] = issue_number
            result_hash['issue_date'] = issue_date
            result_hash['table_of_content'] = table_of_content
            result_hash['content'] = content


            return result_hash 

    def get_article_id(issue_number, db = dbConnect()):
            
        article_id = db.retreive_article_id(issue_number)
        return article_id
        

    def insert_new_newsletter(result_hash):
        db = dbConnect()

            
        article_id = crawling.get_article_id(result_hash['issue_number'], db)
            
        # ì´ë¯¸ ë°ì´í„°ê°€ ë“¤ì–´ìˆìœ¼ë©´ ë„£ì§€ì•Šê¸° 
        if article_id is None: 
            db.insertArchiveData(result_hash['title'], result_hash['issue_number'], result_hash['issue_date'], result_hash['table_of_content'], result_hash['content'])
            article_id = crawling.get_article_id(result_hash['issue_number'], db)
            if result_hash['tags'] is not None: 
                # tag ë°ì´í„° ì§‘ì–´ë„£ê¸° 
                tag_id_list = db.insert_tags_to_tag_table(result_hash['tags']) 
                db.insert_tags_id_to_tag_article_table(tag_id_list, article_id)
                crawling.close_db(db)
                return "success"
        else: 
              return "already exist"
        
    def update_issue_date(result_hash):
        db = dbConnect()

        article_id = crawling.get_article_id(result_hash['issue_number'])
        db.update_issue_date(result_hash['issue_date'], article_id)
        crawling.close_db(db)

    def update_table_of_content(result_hash):
        db = dbConnect()

        article_id = crawling.get_article_id(result_hash['issue_number'])
        db.update_table_of_content(result_hash['table_of_content'], article_id)
        crawling.close_db(db)

    def update_content(result_hash):
        db = dbConnect()
        article_id = crawling.get_article_id(result_hash['issue_number'])
        db.update_table_of_content(result_hash['content'], article_id)
        crawling.close_db(db)
        

    def close_db(db):
        db.closeDB()


    

            


#     
# 43, 18, 69 = ì£¼ìš” ì†Œì‹ 
# 105, 72 = ì˜¤ëŠ˜ì˜ ê¹€ì¹˜ì•¤ì¹©ìŠ¤ 
